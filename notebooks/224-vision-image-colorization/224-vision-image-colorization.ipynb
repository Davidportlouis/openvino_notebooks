{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1662ee2c-fdcd-4b68-a65d-3b03a9e6aadf",
   "metadata": {},
   "source": [
    "### Image Colorization with OpenVino\n",
    "\n",
    "This notebook demonstrates how to colorize images with OpenVINO using the Colorization model [colorization-v2](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/public/colorization-v2/README.md) or [colorization-siggraph](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/colorization-siggraph) from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/public/index.md) based on the paper [Colorful Image Colorization](https://arxiv.org/abs/1603.08511).\n",
    "\n",
    "![Let there be color](data/banner.png)\n",
    "\n",
    "\n",
    "The idea here is given a grayscale image as input, the model hallucinates a plausible, vibrant & realistic colorized version of the image.\n",
    "\n",
    "#### About Colorization-v2\n",
    "* The colorization-v2 model is one of the colorization group of models designed to perform image colorization.\n",
    "* Model was trained on ImageNet dataset.\n",
    "* Model consumes as input L-channel of LAB-image and give as output predict A- and B-channels of LAB-image.\n",
    "\n",
    "#### About Colorization-siggraph\n",
    "* The colorization-siggraph model is one of the colorization group of models designed to real-time user-guided image colorization.\n",
    "* Model was trained on ImageNet dataset with synthetically generated user interaction.\n",
    "* Model consumes as input L-channel of LAB-image and yields output predict A- and B-channels of LAB-image.\n",
    "\n",
    "Check out [colorization](https://github.com/richzhang/colorization) repository for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae449c6-8a58-4d2c-8389-fde359ad7d1a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd439f78-a4b6-48b2-946e-a308383cf787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openvino.runtime import Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6ca64-aa95-4e50-b5a6-975259d16dcc",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "* `PRECISION` - {FP16, FP32}, default: FP16 \n",
    "* `MODEL_DIR` - directory where the model is to be stored, default: public.\n",
    "* `MODEL_NAME` - name of the model used for inference, default: colorization-v2\n",
    "* `DATA_DIR` - directory where test images are stored, default: data\n",
    "* `DEVICE` - {CPU, GPU, GNA,VPU} device to used for inference, default: CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc1b06-b3ae-40f2-8fca-1368455a369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECISION = \"FP16\"\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_NAME = \"colorization-v2\"\n",
    "# MODEL_NAME=\"colorization-siggraph\"\n",
    "MODEL_PATH = f\"{MODEL_DIR}/public/{MODEL_NAME}/{PRECISION}/{MODEL_NAME}.xml\"\n",
    "DATA_DIR = \"data\"\n",
    "DEVICE = \"CPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf87170-fd22-4fcf-b687-51b404be0db4",
   "metadata": {},
   "source": [
    "## Download the model\n",
    "\n",
    "`omz_downloader` downloads model files from online sources and, if necessary, patches them to make them more usable with Model Convertor.\n",
    "\n",
    "In our case `omz_downloader` downloads the checkpoint and pytorch model of [colorization-v2](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/public/colorization-v2/README.md) or [colorization-siggraph](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/colorization-siggraph) from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/public/index.md) and saves it under `MODEL_DIR` as specified in above configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a06d8-d0ec-440f-ae7a-afc4ebe841e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_command = (\n",
    "    f\"omz_downloader \"\n",
    "    f\"--name {MODEL_NAME} \"\n",
    "    f\"--output_dir {MODEL_DIR} \"\n",
    "    f\"--cache_dir {MODEL_DIR}\"\n",
    ")\n",
    "! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93715d09-a6b9-4af5-b129-462c0cb9fc27",
   "metadata": {},
   "source": [
    "## Convert the model to OpenVINO IR\n",
    "\n",
    "`omz_converter` converts the models that are not in the OpenVINOâ„¢ IR format into that format using Model Optimizer.\n",
    "\n",
    "Our downloaded pytorch model is not in OpenVINO IR format which is required for inference with OpenVINO runtime, `omz_converter` is used to convert our downloaded pytorch model into ONNX and OpenVINO IR format respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb56dd-0bd5-4a27-8588-91fb79a9b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    convert_command = (\n",
    "        f\"omz_converter \"\n",
    "        f\"--name {MODEL_NAME} \"\n",
    "        f\"--download_dir {MODEL_DIR} \"\n",
    "        f\"--precisions {PRECISION}\"\n",
    "    )\n",
    "    ! $convert_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f0b29-dd61-469e-86e8-d7fb956a636c",
   "metadata": {},
   "source": [
    "## Loading the Model\n",
    "Load the model in Inference Engine with `ie.read_model` and compile it for the specified device with `ie.compile_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc5285-1059-438b-80ef-7f2021b0e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "model = ie.read_model(model=MODEL_PATH)\n",
    "compiled_model = ie.compile_model(model=model, device_name=DEVICE)\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "N, C, H, W = list(input_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7681922b-5be1-4ec0-b48c-39af5e7bb44b",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47519c1-a6dd-468b-845d-96386033e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(impath: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns an image as ndarra, given path to an image reads the\n",
    "    (BGR) image using opencv's imread() API.\n",
    "\n",
    "        Parameter:\n",
    "            impath (string): Path of the image to be read and returned.\n",
    "\n",
    "        Returns:\n",
    "            image (ndarray): Numpy array representing the read image.\n",
    "    \"\"\"\n",
    "\n",
    "    raw_image = cv2.imread(impath)\n",
    "    if raw_image.shape[2] > 1:\n",
    "        image = cv2.cvtColor(\n",
    "            cv2.cvtColor(raw_image, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2RGB\n",
    "        )\n",
    "    else:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def plot_image(image: np.ndarray, title: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Given a image as ndarray and title as string, display it using\n",
    "    matplotlib.\n",
    "\n",
    "        Parameters:\n",
    "            image (ndarray): Numpy array representing the image to be\n",
    "                             displayed.\n",
    "            title (string): String representing the title of the plot.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_output(gray_img: np.ndarray, color_img: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Plots the original (bw or grayscale) image and colorized image\n",
    "    on different column axes for comparing side by side.\n",
    "\n",
    "        Parameters:\n",
    "            gray_image (ndarray): Numpy array representing the original image.\n",
    "            color_image (ndarray): Numpy array representing the model output.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    plt.title(\"Input\", fontsize=20)\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    plt.title(\"Colorized\", fontsize=20)\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    ax1.imshow(gray_img)\n",
    "    ax2.imshow(color_img)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1948813-77e6-434d-967a-5e0940cb40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_0 = read_image(\"data/test_0.jpg\")\n",
    "test_img_1 = read_image(\"data/test_1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d8e4c-9baa-4859-b9dd-c87b49a1385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_request = compiled_model.create_infer_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afccb409-06a0-4e94-8f8a-779628425a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(gray_img: np.ndarray, ir: object) -> np.ndarray:\n",
    "\n",
    "    \"\"\"\n",
    "    Given an image as ndarray for inference with inference request\n",
    "    convert the image into LAB image, the model consumes as input\n",
    "    L-Channel of LAB image and provides output A & B - Channels of\n",
    "    LAB image. i.e returns a colorized image\n",
    "\n",
    "        Parameters:\n",
    "            gray_img (ndarray): Numpy array representing the original\n",
    "                                image.\n",
    "            ir (object): Inference request created from the compiled\n",
    "                         openVINO model.\n",
    "\n",
    "        Returns:\n",
    "            colorize_image (ndarray): Numpy arrray depicting the\n",
    "                                      colorized version of the original\n",
    "                                      image.\n",
    "    \"\"\"\n",
    "    inputs = {}\n",
    "    for input in model.inputs:\n",
    "        inputs[input.get_any_name()] = np.zeros(input.shape)\n",
    "\n",
    "    # Preprocess\n",
    "    h_in, w_in, _ = gray_img.shape\n",
    "    img_rgb = gray_img.astype(np.float32) / 255\n",
    "    img_lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2Lab)\n",
    "    img_l_rs = cv2.resize(img_lab.copy(), (W, H))[:, :, 0]\n",
    "\n",
    "    # Inference\n",
    "    inputs[\"data_l\"] = np.expand_dims(img_l_rs, axis=[0, 1])\n",
    "    res = infer_request.infer(inputs)[output_layer]\n",
    "    update_res = np.squeeze(res)\n",
    "\n",
    "    # Post-process\n",
    "    out = update_res.transpose((1, 2, 0))\n",
    "    out = cv2.resize(out, (w_in, h_in))\n",
    "    img_lab_out = np.concatenate((img_lab[:, :, 0][:, :, np.newaxis],\n",
    "                                  out), axis=2)\n",
    "    img_bgr_out = np.clip(cv2.cvtColor(img_lab_out, cv2.COLOR_Lab2RGB), 0, 1)\n",
    "    colorized_image = (cv2.resize(img_bgr_out, (w_in, h_in))\n",
    "                       * 255).astype(np.uint8)\n",
    "    return colorized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2da5e-9059-4220-bb58-a1283fa7aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_img_0 = colorize(test_img_0, infer_request)\n",
    "color_img_1 = colorize(test_img_1, infer_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a950c4-7b94-4f84-82c7-3be7fddcc8e0",
   "metadata": {},
   "source": [
    "## Display Colorized Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7e75f-45a6-4ced-aa0d-29f707ddf55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_img_0, color_img_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f7cc9-b872-4d9a-a144-36480cb88105",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(test_img_1, color_img_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9339bc15-9089-4ac7-84b4-8cca2f5dc63e",
   "metadata": {},
   "source": [
    "### Video Colorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce382eb4-61dd-4f66-9f9f-2b110ab8295c",
   "metadata": {
    "tags": [],
    "test_replace": {
     "while cap.isOpened()": "while cap.isOpened() and i <= 1"
    }
   },
   "outputs": [],
   "source": [
    "def colorize_video(data_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Given a path for video input file, colorize each frame\n",
    "    of video and save the colorized output video.\n",
    "\n",
    "        Parameters:\n",
    "            data_path (str): Local path for input video to be\n",
    "                             colorized\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    output_dir = \"out\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    output_path = f\"{output_dir}/{os.path.basename(data_path)}\"\n",
    "\n",
    "    cap = cv2.VideoCapture(data_path)\n",
    "    if not cap.isOpened():\n",
    "        return\n",
    "\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(\"M\", \"P\", \"4\", \"V\")\n",
    "    writer = cv2.VideoWriter(output_path, fourcc, 29.7, (w, h))\n",
    "    i = 0\n",
    "    elapsed = time.time()\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            colorized_frame = colorize(frame, infer_request)\n",
    "            writer.write(colorized_frame)\n",
    "        if i % 1000 == 0:\n",
    "            print(\n",
    "                f\"frames processed: {i} - elpased_time: \"\n",
    "                f\"{time.time() - elapsed} secs\"\n",
    "            )\n",
    "            elapsed = time.time()\n",
    "        i += 1\n",
    "\n",
    "    if cap:\n",
    "        cap.release()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb763e-9b3e-495b-a264-b3d460317881",
   "metadata": {},
   "outputs": [],
   "source": [
    "colorize_video(\"data/NY_BW.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
